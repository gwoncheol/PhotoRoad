#ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì‚¬ìš©í•´ì„œ ìœ ì‚¬ë„ ê³„ì‚° í›„ ì¶”ì²œì²œ
import os
import csv
import torch
import clip
from PIL import Image
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import time
import pickle  # ì„ë² ë”© ì €ì¥ìš©

# ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì‹œì‘
total_start_time = time.time()

# CLIP ëª¨ë¸ ë¡œë“œ
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device)

# CSV íŒŒì¼ ê²½ë¡œ
csv_file = "labels.csv"
embedding_cache_file = "image_embeddings.pkl"

# ì´ë¯¸ì§€ ê²½ë¡œ ë° í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
image_paths = []
text_labels = []

# CSV íŒŒì¼ ì½ê¸°
with open(csv_file, newline="", encoding="utf-8") as f:
    reader = csv.reader(f)
    next(reader)  # í—¤ë” ê±´ë„ˆë›°ê¸°
    for row in reader:
        image_paths.append(row[0])
        text_labels.append(row[1])

# í…ìŠ¤íŠ¸ ë¼ë²¨ì„ CLIP ëª¨ë¸ì— ì…ë ¥ ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
text_inputs = clip.tokenize(text_labels).to(device)
with torch.no_grad():
    text_features = model.encode_text(text_inputs)

# ì´ë¯¸ì§€ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° ë° ì €ì¥
if not os.path.exists(embedding_cache_file):
    print("â³ ì´ë¯¸ì§€ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° ì¤‘...")
    image_features = {}
    for path in image_paths:
        image = preprocess(Image.open(path)).unsqueeze(0).to(device)
        with torch.no_grad():
            feat = model.encode_image(image).cpu().numpy()
        image_features[path] = feat
    # ì„ë² ë”© ì €ì¥
    with open(embedding_cache_file, "wb") as f:
        pickle.dump(image_features, f)
    print("âœ… ì´ë¯¸ì§€ ì„ë² ë”© ì €ì¥ ì™„ë£Œ.")
else:
    print("âœ… ì €ì¥ëœ ì´ë¯¸ì§€ ì„ë² ë”© ë¡œë”© ì¤‘...")
    with open(embedding_cache_file, "rb") as f:
        image_features = pickle.load(f)

# ìœ ì‚¬ë„ ê¸°ë°˜ ì´ë¯¸ì§€ ì¶”ì²œ (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì„ë² ë”© ì¡°í•©)
def recommend_similar_images_with_text(user_image_path, top_k=5, alpha=0.5):
    """
    alpha: ì´ë¯¸ì§€ ìœ ì‚¬ë„ ë¹„ì¤‘ (0.0 ~ 1.0). ë‚˜ë¨¸ì§€ëŠ” í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ì— í• ë‹¹.
    """
    # ì‚¬ìš©ì ì´ë¯¸ì§€ ì„ë² ë”© ê³„ì‚°
    image = preprocess(Image.open(user_image_path)).unsqueeze(0).to(device)
    with torch.no_grad():
        user_image_feat = model.encode_image(image).cpu().numpy()
        user_image_feat /= np.linalg.norm(user_image_feat)

        # í…ìŠ¤íŠ¸ ì„ë² ë”©ë“¤ê³¼ ë¹„êµí•˜ì—¬ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
        user_text_sim = cosine_similarity(user_image_feat, text_features.cpu().numpy())[0]
    
    similarities = []
    for i, (path, img_feat) in enumerate(image_features.items()):
        img_feat_norm = img_feat / np.linalg.norm(img_feat)

        # ì´ë¯¸ì§€ ìœ ì‚¬ë„
        img_sim = cosine_similarity(user_image_feat, img_feat_norm)[0][0]
        # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: í•´ë‹¹ ì´ë¯¸ì§€ì˜ ë¼ë²¨ ê¸°ì¤€
        txt_sim = user_text_sim[i]

        # ì´ë¯¸ì§€ ìœ ì‚¬ë„ì™€ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì¡°í•©
        total_sim = alpha * img_sim + (1 - alpha) * txt_sim
        similarities.append((path, total_sim, text_labels[i]))

    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]

# ì‹¤í–‰
user_image_path = "TestImage/ì˜¤í”„ë¡œë“œ.jpg"  # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ

start_time = time.time()
recommended_images = recommend_similar_images_with_text(user_image_path, top_k=5, alpha=0.5)
end_time = time.time()

print("\nğŸ“Œ Recommended Images:")
for img, sim, txt in recommended_images:
    print(f"Image: {img}, Label: {txt}, Similarity: {sim:.4f}")

print(f"\nğŸ•’ Recommendation step time: {end_time - start_time:.2f} seconds")

# ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¢…ë£Œ
total_end_time = time.time()
print(f"âœ… Total execution time: {total_end_time - total_start_time:.2f} seconds")
